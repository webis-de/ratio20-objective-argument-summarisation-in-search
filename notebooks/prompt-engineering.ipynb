{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import openai\n",
    "config = dotenv.dotenv_values(\"../.env\")\n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import json\n",
    "import textwrap\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/appropriateness-corpus/inappropriate_with_reasons_conservative_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = df['argument'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(arguments)\n",
    "textwrap.wrap(sample, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[['issue', 'argument', 'Reasons', 'word_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"original_argument\": {\"type\": \"string\", \"description\": \"The original argument from the user\"},\n",
    "        \"topic\": {\"type\": \"string\", \"description\": \"The topic of the argument\"},\n",
    "        \"transformed_argument\": {\"type\": \"string\", \"description\": \"The transformed argument\"},\n",
    "        \"reasons\": {\"type\": \"array\", \"description\": \"The reasons for the transformation\", \"items\": {\"type\": \"string\"}},\n",
    "        \"transformations\": {\"type\": \"array\", \"description\": \"The tuples of original text spans, their transformations, and the action taken to transform them.\", \"items\": {\"type\": \"object\", \"properties\": {\n",
    "            \"original_text\": {\"type\": \"string\", \"description\": \"The original text span\"},\n",
    "            \"transformed_text\": {\"type\": \"string\", \"description\": \"The transformed text span\"},\n",
    "            \"action\": {\"type\": \"string\", \"description\": \"The action taken to transform the text span\"},\n",
    "        }}}\n",
    "    },\n",
    "    \"required\": [\"original_argument\", \"topic\", \"transformed_argument\", \"reasons\", \"transformations\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_informal_prompt(row):\n",
    "    topic = row['issue']\n",
    "    argument = row['argument']\n",
    "    word_count = int(row['word_count'])\n",
    "    debate_prompt_informal = f\"\"\"\n",
    "    You are participating in an online debate on the topic of \"{topic}\". You are an expert debater, and your task is to check if a given user's argument is appropriate to be presented in a civil debate. \n",
    "\n",
    "    If yes, do nothing and return it. If not, then convert it into an appropriate argument and provide reasons for your conversion. The reasons should be a list of short descriptive phrases. Also return the list of transformations corresponding to each span in the user's argument in the form ('original','transformed', 'action') where 'action' describes in natural language the transformation that you applied.\n",
    "\n",
    "    Next, check if the actions match the reasons. If they do not, adjust your actions to match the reasons and update the list of transformations accordingly.\n",
    "\n",
    "    Please ensure that the semantics of the transformed argument must be very similar to the original one, albeit with small changes to make it appropriate. The transformed argument should be grammatically correct and should not contain any spelling mistakes or facts that cannot be verified against the original argument. The transformed argument should be relevant to the topic of the debate and should be approximately {word_count} words long. \n",
    "\n",
    "    User's argument: \"\n",
    "    {argument}\n",
    "    \"\n",
    "    \"\"\"\n",
    "    row['informal_prompt'] = debate_prompt_informal\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.progress_apply(create_informal_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_arguments_informal_debater(row):\n",
    "    row['system_message'] = {\"role\": \"system\", \"content\": row['informal_prompt']}\n",
    "    try:\n",
    "      completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "          row['system_message'],\n",
    "        ],\n",
    "        functions=[{\"name\": \"set_argument\", \"parameters\": response_schema}],\n",
    "        function_call={\"name\": \"set_argument\"},\n",
    "        temperature=0,\n",
    "      )\n",
    "      response_dict = json.loads(completion.choices[0].message.function_call.arguments)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      response_dict = {}\n",
    "    row['llm_response'] = response_dict\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sub_df.sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"../data/inappropriate_arguments_sample_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neutralized = sample.progress_apply(transform_arguments_informal_debater, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neutralized.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neutralized.to_csv(\"../data/neutralized_sample_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sample_neutralized['llm_response'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = sample_neutralized['llm_response'].tolist()\n",
    "human_reasons = sample_neutralized['Reasons'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty responses\n",
    "responses = [x for x in responses if x != {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if reasons is empty\n",
    "valid_reasons = [x for x in human_reasons if x!= '[]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write only informal arguments\n",
    "with open('../data/neutralized_arguments_sample_50.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for response, reason in zip(responses, valid_reasons):\n",
    "        record = {}\n",
    "        record['topic'] = response['topic']\n",
    "        record['argument'] = response['original_argument']\n",
    "        record['neutralized'] = response['transformed_argument']\n",
    "        record['model_reasons'] = response['reasons']\n",
    "        record['model_transformations'] = response['transformations']\n",
    "        record['human_reasons'] = reason\n",
    "        json.dump(record, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
